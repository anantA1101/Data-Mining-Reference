{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Association pattern mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apriori algorithm and an algorithm for generating association rules.\n",
    "\n",
    "i will apply the implemented algorithms to Titanic dataset to identify which groups of passengers were likely to survive/not survive in the catastrophe.\n",
    "\n",
    "Each record has 4 features:\n",
    "- Class (0 = crew, 1 = first, 2 = second, 3 = third)\n",
    "- Age   (1 = adult, 0 = child)\n",
    "- Sex   (1 = male, 0 = female)\n",
    "- Survived (1 = yes, 0 = no)\n",
    "\n",
    "For the Frequent Pattern Mining Model and Apriori algorithm we need to convert our dataset to a binary dataset using [one-hot encoding] \n",
    "\n",
    "Using onew-hot encoding, the original four features of our dataset will be converted into the following 10 binary features:\n",
    "- Crew\n",
    "- 1st class\n",
    "- 2nd class\n",
    "- 3rd class\n",
    "- Child\n",
    "- Adult\n",
    "- Female\n",
    "- Male\n",
    "- Survived\n",
    "- Not survived"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Read Titanic dataset from the input file and transform it using one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and convert dataset using one-hot encoding\n",
    "def load_data_one_hot(fname):\n",
    "    itemNames = ['Crew', '1st class', '2nd class', '3rd class', 'Child', 'Adult', 'Female', 'Male', 'Survived', 'Not survived']\n",
    "    dataset = []\n",
    "    \n",
    "    with open(fname) as F:\n",
    "        next(F) # skip the first line with feature names\n",
    "        for line in F:\n",
    "            p = line.strip().split('        ')\n",
    "            X = np.array(p, int)\n",
    "            newX = [0]*10\n",
    "            if X[0] == 0: newX[0] = 1 # Crew\n",
    "            if X[0] == 1: newX[1] = 1 # First class\n",
    "            if X[0] == 2: newX[2] = 1 # Second class\n",
    "            if X[0] == 3: newX[3] = 1 # Thirds class\n",
    "            if X[1] == 0: newX[4] = 1 # Child\n",
    "            if X[1] == 1: newX[5] = 1 # Adult\n",
    "            if X[2] == 0: newX[6] = 1 # Female\n",
    "            if X[2] == 1: newX[7] = 1 # Male   \n",
    "            if X[3] == 1: newX[8] = 1 # Survived\n",
    "            if X[3] == 0: newX[9] = 1 # Not survived\n",
    "            dataset.append(newX)    \n",
    "    \n",
    "    return np.array(itemNames), np.array(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Crew' '1st class' '2nd class' '3rd class' 'Child' 'Adult' 'Female'\n",
      " 'Male' 'Survived' 'Not survived']\n",
      "[[0 1 0 ... 1 1 0]\n",
      " [0 1 0 ... 1 1 0]\n",
      " [0 1 0 ... 1 1 0]\n",
      " ...\n",
      " [1 0 0 ... 0 0 1]\n",
      " [1 0 0 ... 0 0 1]\n",
      " [1 0 0 ... 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "itemNames, dataset = load_data_one_hot('titanic.dat.txt')\n",
    "print(itemNames)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. Implement Apriori algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computes support of a given itemset\n",
    "def sup(dataset, itemset):\n",
    "    # compute the number of items in the itemset\n",
    "    numItems = len(itemset)\n",
    "    # for every instance consider only items from itemset\n",
    "    resDataset = dataset[:,itemset]\n",
    "    \n",
    "    return np.sum(np.sum(resDataset, axis=1) == numItems)/len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given the frequent (k-1)-itemsets, generate the candidate k-itemsets\n",
    "def generateCandidates(F, k, d):\n",
    "    # Create an empty list of candidate k-itemsets\n",
    "    C = []\n",
    "\n",
    "    ### JOIN PHASE\n",
    "    # for each frequent itemset\n",
    "    for itemset in F:\n",
    "        # and for every i in the range from the value of the last element in the itemset + 1 to d (the number of elements in the universe)\n",
    "        for i in range(itemset[-1]+1,d+1):\n",
    "            # construct itemset1 by replacing the last element in the itemset with i\n",
    "            itemset1 = itemset[:-1] + [i]\n",
    "            # if itemset1 is frequent k-itemset\n",
    "            if itemset1 in F:\n",
    "                # add (itemset union {i}) to the collection of candidate k-itemsets\n",
    "                C.append(itemset+[i])\n",
    "\n",
    "    ### PRUNE PHASE\n",
    "    # for each candidate k-itemset\n",
    "    for itemset in C:\n",
    "        # check if it satisfies the Downward Closure Property\n",
    "        for i in itemset:\n",
    "            tempItemset = itemset.copy()\n",
    "            tempItemset.remove(i)\n",
    "            if tempItemset not in F:\n",
    "                C.remove(itemset)\n",
    "                break\n",
    "            \n",
    "    return C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate all frequent itemsets for a given frequency threshold\n",
    "def apriori(dataset, freqThreshold):\n",
    "    # compute the number of instances in the dataset\n",
    "    N = len(dataset)\n",
    "    # compute the number items in the universe\n",
    "    d = len(dataset[0])\n",
    "\n",
    "    # Frequent itemsets will be stored in a dictionary. Key -- size of itemsets, Value -- all frequent itemsets of that size\n",
    "    F = {i+1 : [] for i in range(d)}\n",
    "\n",
    "    # Compute frequent 1-itemsets\n",
    "    for i in range(d):\n",
    "        if sup(dataset, [i]) >= freqThreshold:\n",
    "            F[1].append([i])\n",
    "\n",
    "    # For every itemsets size from 2 to d\n",
    "    for k in range(2,d+1):\n",
    "        # If there are no frequent (k-1)-itemsets, then STOP\n",
    "        if len(F[k-1]) == 0:\n",
    "            break\n",
    "        # Otherwise generate k-itemsets candidates\n",
    "        Ck = generateCandidates(F[k-1], k, d)\n",
    "        # and for every candidate k-itemset \n",
    "        for itemset in Ck:\n",
    "            # if it has sufficiently large support\n",
    "            if sup(dataset, itemset) >= freqThreshold:\n",
    "                # add it to the collections of the frequent k-itemsets\n",
    "                F[k].append(itemset)\n",
    "\n",
    "    return F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Generate and print all frequent itemsets for the frequency threshold 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequent 1-itemset: [[0], [1], [2], [3], [4], [5], [6], [7], [8], [9]]\n",
      "Frequent 2-itemset: [[0, 5], [0, 6], [0, 7], [0, 8], [0, 9], [1, 5], [1, 6], [1, 7], [1, 8], [1, 9], [2, 4], [2, 5], [2, 6], [2, 7], [2, 8], [2, 9], [3, 4], [3, 5], [3, 6], [3, 7], [3, 8], [3, 9], [4, 6], [4, 7], [4, 8], [4, 9], [5, 6], [5, 7], [5, 8], [5, 9], [6, 8], [6, 9], [7, 8], [7, 9]]\n",
      "Frequent 3-itemset: [[0, 5, 6], [0, 5, 7], [0, 5, 8], [0, 5, 9], [0, 6, 8], [0, 7, 8], [0, 7, 9], [1, 5, 6], [1, 5, 7], [1, 5, 8], [1, 5, 9], [1, 6, 8], [1, 7, 8], [1, 7, 9], [2, 4, 6], [2, 4, 7], [2, 4, 8], [2, 5, 6], [2, 5, 7], [2, 5, 8], [2, 5, 9], [2, 6, 8], [2, 6, 9], [2, 7, 8], [2, 7, 9], [3, 4, 6], [3, 4, 7], [3, 4, 8], [3, 4, 9], [3, 5, 6], [3, 5, 7], [3, 5, 8], [3, 5, 9], [3, 6, 8], [3, 6, 9], [3, 7, 8], [3, 7, 9], [4, 6, 8], [4, 6, 9], [4, 7, 8], [4, 7, 9], [5, 6, 8], [5, 6, 9], [5, 7, 8], [5, 7, 9]]\n",
      "Frequent 4-itemset: [[0, 5, 6, 8], [0, 5, 7, 8], [0, 5, 7, 9], [1, 5, 6, 8], [1, 5, 7, 8], [1, 5, 7, 9], [2, 4, 6, 8], [2, 4, 7, 8], [2, 5, 6, 8], [2, 5, 6, 9], [2, 5, 7, 8], [2, 5, 7, 9], [3, 4, 6, 8], [3, 4, 6, 9], [3, 4, 7, 8], [3, 4, 7, 9], [3, 5, 6, 8], [3, 5, 6, 9], [3, 5, 7, 8], [3, 5, 7, 9]]\n"
     ]
    }
   ],
   "source": [
    "F = apriori(dataset, 0.005)\n",
    "for k,val in F.items():\n",
    "    if len(val) > 0:\n",
    "        print('Frequent %d-itemset:' % (k), val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. Implement an algorithm that takes frequent itemsets as an input and generate from these itemsets all association rules at a given confidence threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate all subsets of a given set s, except the empty set and s itself (the function is implemented as generator, which means that we can use it as iterator)\n",
    "def powerset(s):\n",
    "    x = len(s)\n",
    "    masks = [1 << i for i in range(x)]\n",
    "    for i in range(1, (1 << x)-1):\n",
    "        yield [ss for mask, ss in zip(masks, s) if i & mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given frequent itemsets, generate the association rules at the given confidence threshold\n",
    "def generateRules(dataset, freqItemsets, confThreshold):\n",
    "    associationRules = []\n",
    "\n",
    "    # for every frequent itemset\n",
    "    for itemset in freqItemsets:\n",
    "        # of size at least 2\n",
    "        if len(itemset) >=2:\n",
    "            # and for every subset X of itemset\n",
    "            for X in powerset(itemset):\n",
    "                # if the confidence of X => Y (where Y is the complement of X) is at least the confidence threshold\n",
    "                support = sup(dataset, itemset)\n",
    "                conf = support/sup(dataset, X)\n",
    "                if conf >= confThreshold:\n",
    "                    # then generate association rule X => Y\n",
    "                    Y = [e for e in itemset if e not in X]\n",
    "                    associationRules.append((X,Y,support,conf))\n",
    "    return associationRules\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Generate and print all association rules at the frequency threshold 0.005 and the confidence threshold 0.8. For every rule, output its support and confidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print association rules\n",
    "def printAssociationRules(associationRules, itemNames):\n",
    "    i = 1\n",
    "    print(\"Support \\t\\t Confidence \\t\\t Association rule\")\n",
    "    for (X,Y,support,conf) in associationRules:\n",
    "        #print(\"%d. { %s } => { %s } \\t\\t\\t %.4f \\t\\t\\t %.4f\" % i, ','.join(itemNames[X]), ','.join(itemNames[Y]), support, conf)\n",
    "        print(\"%d. %.4f \\t\\t %.4f \\t\\t { %s } => { %s } \\t\\t \" % (i, support, conf, ', '.join(itemNames[X]), ', '.join(itemNames[Y])))\n",
    "        #print(i, '{',itemNames[X],'} => {', itemNames[Y],\"        \", support, )\n",
    "        i = i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Association Rules at the frequency threshold 0.005 and the confidence threshold 0.8\n",
      "\n",
      "Support \t\t Confidence \t\t Association rule\n",
      "1. 0.4023 \t\t 1.0000 \t\t { Crew } => { Adult } \t\t \n",
      "2. 0.3918 \t\t 0.9740 \t\t { Crew } => { Male } \t\t \n",
      "3. 0.1445 \t\t 0.9815 \t\t { 1st class } => { Adult } \t\t \n",
      "4. 0.1186 \t\t 0.9158 \t\t { 2nd class } => { Adult } \t\t \n",
      "5. 0.2850 \t\t 0.8881 \t\t { 3rd class } => { Adult } \t\t \n",
      "6. 0.1932 \t\t 0.9043 \t\t { Female } => { Adult } \t\t \n",
      "7. 0.7573 \t\t 0.9630 \t\t { Male } => { Adult } \t\t \n",
      "8. 0.2968 \t\t 0.9197 \t\t { Survived } => { Adult } \t\t \n",
      "9. 0.6536 \t\t 0.9651 \t\t { Not survived } => { Adult } \t\t \n",
      "10. 0.6200 \t\t 0.9154 \t\t { Not survived } => { Male } \t\t \n",
      "11. 0.0105 \t\t 1.0000 \t\t { Crew, Female } => { Adult } \t\t \n",
      "12. 0.3918 \t\t 0.9740 \t\t { Crew } => { Adult, Male } \t\t \n",
      "13. 0.3918 \t\t 0.9740 \t\t { Crew, Adult } => { Male } \t\t \n",
      "14. 0.3918 \t\t 1.0000 \t\t { Crew, Male } => { Adult } \t\t \n",
      "15. 0.0964 \t\t 1.0000 \t\t { Crew, Survived } => { Adult } \t\t \n",
      "16. 0.3059 \t\t 1.0000 \t\t { Crew, Not survived } => { Adult } \t\t \n",
      "17. 0.0091 \t\t 0.8696 \t\t { Crew, Female } => { Survived } \t\t \n",
      "18. 0.0873 \t\t 0.9057 \t\t { Crew, Survived } => { Male } \t\t \n",
      "19. 0.3045 \t\t 0.9955 \t\t { Crew, Not survived } => { Male } \t\t \n",
      "20. 0.0655 \t\t 0.9931 \t\t { 1st class, Female } => { Adult } \t\t \n",
      "21. 0.0791 \t\t 0.9721 \t\t { 1st class, Male } => { Adult } \t\t \n",
      "22. 0.0891 \t\t 0.9703 \t\t { 1st class, Survived } => { Adult } \t\t \n",
      "23. 0.0555 \t\t 1.0000 \t\t { 1st class, Not survived } => { Adult } \t\t \n",
      "24. 0.0641 \t\t 0.9724 \t\t { 1st class, Female } => { Survived } \t\t \n",
      "25. 0.0536 \t\t 0.9672 \t\t { 1st class, Not survived } => { Male } \t\t \n",
      "26. 0.0109 \t\t 1.0000 \t\t { 2nd class, Child } => { Survived } \t\t \n",
      "27. 0.0423 \t\t 0.8774 \t\t { 2nd class, Female } => { Adult } \t\t \n",
      "28. 0.0764 \t\t 0.9385 \t\t { 2nd class, Male } => { Adult } \t\t \n",
      "29. 0.0759 \t\t 1.0000 \t\t { 2nd class, Not survived } => { Adult } \t\t \n",
      "30. 0.0423 \t\t 0.8774 \t\t { 2nd class, Female } => { Survived } \t\t \n",
      "31. 0.0700 \t\t 0.8603 \t\t { 2nd class, Male } => { Not survived } \t\t \n",
      "32. 0.0700 \t\t 0.9222 \t\t { 2nd class, Not survived } => { Male } \t\t \n",
      "33. 0.0236 \t\t 1.0000 \t\t { Child, Not survived } => { 3rd class } \t\t \n",
      "34. 0.0750 \t\t 0.8418 \t\t { 3rd class, Female } => { Adult } \t\t \n",
      "35. 0.2100 \t\t 0.9059 \t\t { 3rd class, Male } => { Adult } \t\t \n",
      "36. 0.0686 \t\t 0.8483 \t\t { 3rd class, Survived } => { Adult } \t\t \n",
      "37. 0.2164 \t\t 0.9015 \t\t { 3rd class, Not survived } => { Adult } \t\t \n",
      "38. 0.0482 \t\t 0.8413 \t\t { Female, Not survived } => { 3rd class } \t\t \n",
      "39. 0.1918 \t\t 0.8275 \t\t { 3rd class, Male } => { Not survived } \t\t \n",
      "40. 0.1436 \t\t 0.9186 \t\t { Female, Survived } => { Adult } \t\t \n",
      "41. 0.0495 \t\t 0.8651 \t\t { Female, Not survived } => { Adult } \t\t \n",
      "42. 0.1532 \t\t 0.9208 \t\t { Male, Survived } => { Adult } \t\t \n",
      "43. 0.6041 \t\t 0.8919 \t\t { Not survived } => { Adult, Male } \t\t \n",
      "44. 0.6041 \t\t 0.9242 \t\t { Adult, Not survived } => { Male } \t\t \n",
      "45. 0.6041 \t\t 0.9743 \t\t { Male, Not survived } => { Adult } \t\t \n",
      "46. 0.0091 \t\t 0.8696 \t\t { Crew, Female } => { Adult, Survived } \t\t \n",
      "47. 0.0091 \t\t 0.8696 \t\t { Crew, Adult, Female } => { Survived } \t\t \n",
      "48. 0.0091 \t\t 1.0000 \t\t { Crew, Female, Survived } => { Adult } \t\t \n",
      "49. 0.0873 \t\t 0.9057 \t\t { Crew, Survived } => { Adult, Male } \t\t \n",
      "50. 0.0873 \t\t 0.9057 \t\t { Crew, Adult, Survived } => { Male } \t\t \n",
      "51. 0.0873 \t\t 1.0000 \t\t { Crew, Male, Survived } => { Adult } \t\t \n",
      "52. 0.3045 \t\t 0.9955 \t\t { Crew, Not survived } => { Adult, Male } \t\t \n",
      "53. 0.3045 \t\t 0.9955 \t\t { Crew, Adult, Not survived } => { Male } \t\t \n",
      "54. 0.3045 \t\t 1.0000 \t\t { Crew, Male, Not survived } => { Adult } \t\t \n",
      "55. 0.0636 \t\t 0.9655 \t\t { 1st class, Female } => { Adult, Survived } \t\t \n",
      "56. 0.0636 \t\t 0.9722 \t\t { 1st class, Adult, Female } => { Survived } \t\t \n",
      "57. 0.0636 \t\t 0.9929 \t\t { 1st class, Female, Survived } => { Adult } \t\t \n",
      "58. 0.0255 \t\t 0.9180 \t\t { 1st class, Male, Survived } => { Adult } \t\t \n",
      "59. 0.0536 \t\t 0.9672 \t\t { 1st class, Not survived } => { Adult, Male } \t\t \n",
      "60. 0.0536 \t\t 0.9672 \t\t { 1st class, Adult, Not survived } => { Male } \t\t \n",
      "61. 0.0536 \t\t 1.0000 \t\t { 1st class, Male, Not survived } => { Adult } \t\t \n",
      "62. 0.0059 \t\t 1.0000 \t\t { 2nd class, Child, Female } => { Survived } \t\t \n",
      "63. 0.0050 \t\t 1.0000 \t\t { 2nd class, Child, Male } => { Survived } \t\t \n",
      "64. 0.0364 \t\t 0.8602 \t\t { 2nd class, Adult, Female } => { Survived } \t\t \n",
      "65. 0.0364 \t\t 0.8511 \t\t { 2nd class, Adult, Survived } => { Female } \t\t \n",
      "66. 0.0364 \t\t 0.8602 \t\t { 2nd class, Female, Survived } => { Adult } \t\t \n",
      "67. 0.0059 \t\t 1.0000 \t\t { 2nd class, Female, Not survived } => { Adult } \t\t \n",
      "68. 0.0700 \t\t 0.8603 \t\t { 2nd class, Male } => { Adult, Not survived } \t\t \n",
      "69. 0.0700 \t\t 0.9167 \t\t { 2nd class, Adult, Male } => { Not survived } \t\t \n",
      "70. 0.0700 \t\t 0.9222 \t\t { 2nd class, Not survived } => { Adult, Male } \t\t \n",
      "71. 0.0700 \t\t 0.9222 \t\t { 2nd class, Adult, Not survived } => { Male } \t\t \n",
      "72. 0.0700 \t\t 1.0000 \t\t { 2nd class, Male, Not survived } => { Adult } \t\t \n",
      "73. 0.0077 \t\t 1.0000 \t\t { Child, Female, Not survived } => { 3rd class } \t\t \n",
      "74. 0.0159 \t\t 1.0000 \t\t { Child, Male, Not survived } => { 3rd class } \t\t \n",
      "75. 0.0345 \t\t 0.8444 \t\t { 3rd class, Female, Survived } => { Adult } \t\t \n",
      "76. 0.0405 \t\t 0.8396 \t\t { 3rd class, Female, Not survived } => { Adult } \t\t \n",
      "77. 0.0405 \t\t 0.8165 \t\t { Adult, Female, Not survived } => { 3rd class } \t\t \n",
      "78. 0.0341 \t\t 0.8523 \t\t { 3rd class, Male, Survived } => { Adult } \t\t \n",
      "79. 0.1759 \t\t 0.8377 \t\t { 3rd class, Adult, Male } => { Not survived } \t\t \n",
      "80. 0.1759 \t\t 0.8130 \t\t { 3rd class, Adult, Not survived } => { Male } \t\t \n",
      "81. 0.1759 \t\t 0.9171 \t\t { 3rd class, Male, Not survived } => { Adult } \t\t \n"
     ]
    }
   ],
   "source": [
    "freqItemsets = []\n",
    "for kitemsets in F.values():\n",
    "    for itemset in kitemsets:\n",
    "        if len(itemset) > 0:\n",
    "            freqItemsets.append(itemset)\n",
    "\n",
    "associationRules = generateRules(dataset, freqItemsets, 0.8)\n",
    "print(\"Association Rules at the frequency threshold 0.005 and the confidence threshold 0.8\\n\")\n",
    "printAssociationRules(associationRules, itemNames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Now print only the association rules of the form X => {Survived} or X => {Not survived}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Support \t\t Confidence \t\t Association rule\n",
      "1. 0.0109 \t\t 1.0000 \t\t { 2nd class, Child } => { Survived } \t\t \n",
      "2. 0.0059 \t\t 1.0000 \t\t { 2nd class, Child, Female } => { Survived } \t\t \n",
      "3. 0.0050 \t\t 1.0000 \t\t { 2nd class, Child, Male } => { Survived } \t\t \n",
      "4. 0.0641 \t\t 0.9724 \t\t { 1st class, Female } => { Survived } \t\t \n",
      "5. 0.0636 \t\t 0.9722 \t\t { 1st class, Adult, Female } => { Survived } \t\t \n",
      "6. 0.0700 \t\t 0.9167 \t\t { 2nd class, Adult, Male } => { Not survived } \t\t \n",
      "7. 0.0423 \t\t 0.8774 \t\t { 2nd class, Female } => { Survived } \t\t \n",
      "8. 0.0091 \t\t 0.8696 \t\t { Crew, Female } => { Survived } \t\t \n",
      "9. 0.0091 \t\t 0.8696 \t\t { Crew, Adult, Female } => { Survived } \t\t \n",
      "10. 0.0700 \t\t 0.8603 \t\t { 2nd class, Male } => { Not survived } \t\t \n",
      "11. 0.0364 \t\t 0.8602 \t\t { 2nd class, Adult, Female } => { Survived } \t\t \n",
      "12. 0.1759 \t\t 0.8377 \t\t { 3rd class, Adult, Male } => { Not survived } \t\t \n",
      "13. 0.1918 \t\t 0.8275 \t\t { 3rd class, Male } => { Not survived } \t\t \n"
     ]
    }
   ],
   "source": [
    "# We are interested only in the association rules of the form X => {Survived} or X => {Not survived}\n",
    "associationRulesSurvived = []\n",
    "for (X,Y,support,conf) in associationRules:\n",
    "    if (Y == [8]) or (Y == [9]):\n",
    "        associationRulesSurvived.append((X,Y,support,conf))\n",
    "\n",
    "# Sort rules with respect to confidence\n",
    "associationRulesSurvived = sorted(associationRulesSurvived, key=lambda x: x[3], reverse=True)\n",
    "printAssociationRules(associationRulesSurvived, itemNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
