{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Association pattern mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab we will implement Apriori algorithm and an algorithm for generating association rules.\n",
    "\n",
    "We will apply the implemented algorithms to Titanic dataset to identify which groups of passengers were likely to survive/not survive in the catastrophe.\n",
    "\n",
    "The original dataset is contained in file **titanic.dat.txt**. Each record has 4 features:\n",
    "- Class (0 = crew, 1 = first, 2 = second, 3 = third)\n",
    "- Age   (1 = adult, 0 = child)\n",
    "- Sex   (1 = male, 0 = female)\n",
    "- Survived (1 = yes, 0 = no)\n",
    "\n",
    "For the Frequent Pattern Mining Model and Apriori algorithm we need to convert our dataset to a binary dataset using [one-hot encoding](https://en.wikipedia.org/wiki/One-hot). In such encoding, for every value of a feature we create a new binary feature that indicates whether the original feature takes this specific value or not.\n",
    "\n",
    "Using onew-hot encoding, the original four features of our dataset will be converted into the following 10 binary features:\n",
    "- Crew\n",
    "- 1st class\n",
    "- 2nd class\n",
    "- 3rd class\n",
    "- Child\n",
    "- Adult\n",
    "- Female\n",
    "- Male\n",
    "- Survived\n",
    "- Not survived"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "Read Titanic dataset from the input file and transform it using one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and convert dataset using one-hot encoding\n",
    "def load_data_one_hot(fname):\n",
    "    itemNames = ['Crew', '1st class', '2nd class', '3rd class', 'Child', 'Adult', 'Female', 'Male', 'Survived', 'Not survived']\n",
    "    dataset = []\n",
    "    \n",
    "    with open(fname) as F:\n",
    "        next(F) # skip the first line with feature names\n",
    "        for line in F:\n",
    "            p = line.strip().split('        ')\n",
    "            X = np.array(p, int)\n",
    "            newX = [0]*10\n",
    "            if X[0] == 0: newX[0] = 1 # Crew\n",
    "            if X[0] == 1: newX[1] = 1 # First class\n",
    "            if X[0] == 2: newX[2] = 1 # Second class\n",
    "            if X[0] == 3: newX[3] = 1 # Thirds class\n",
    "            if X[1] == 0: newX[4] = 1 # Child\n",
    "            if X[1] == 1: newX[5] = 1 # Adult\n",
    "            if X[2] == 0: newX[6] = 1 # Female\n",
    "            if X[2] == 1: newX[7] = 1 # Male   \n",
    "            if X[3] == 1: newX[8] = 1 # Survived\n",
    "            if X[3] == 0: newX[9] = 1 # Not survived\n",
    "            dataset.append(newX)    \n",
    "    \n",
    "    return np.array(itemNames), np.array(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itemNames, dataset = load_data_one_hot('titanic.dat.txt')\n",
    "print(itemNames)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "\n",
    "1. Implement Apriori algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computes support of a given itemset\n",
    "def sup(dataset, itemset):\n",
    "    # compute the number of items in the itemset\n",
    "    numItems = len(itemset)\n",
    "    # for every instance consider only items from itemset\n",
    "    resDataset = dataset[:,itemset]\n",
    "    \n",
    "    return np.sum(np.sum(resDataset, axis=1) == numItems)/len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given the frequent (k-1)-itemsets, generate the candidate k-itemsets\n",
    "def generateCandidates(F, k, d):\n",
    "    # Create an empty list of candidate k-itemsets\n",
    "    C = []\n",
    "\n",
    "    ### JOIN PHASE\n",
    "    # for each frequent itemset\n",
    "    for itemset in F:\n",
    "        # and for every i in the range from the value of the last element in the itemset + 1 to d (the number of elements in the universe)\n",
    "        for i in range(itemset[-1]+1,d+1):\n",
    "            # construct itemset1 by replacing the last element in the itemset with i\n",
    "            itemset1 = itemset[:-1] + [i]\n",
    "            # if itemset1 is frequent k-itemset\n",
    "            if itemset1 in F:\n",
    "                # add (itemset union {i}) to the collection of candidate k-itemsets\n",
    "                C.append(itemset+[i])\n",
    "\n",
    "    ### PRUNE PHASE\n",
    "    # for each candidate k-itemset\n",
    "    for itemset in C:\n",
    "        # check if it satisfies the Downward Closure Property\n",
    "        for i in itemset:\n",
    "            tempItemset = itemset.copy()\n",
    "            tempItemset.remove(i)\n",
    "            if tempItemset not in F:\n",
    "                C.remove(itemset)\n",
    "                break\n",
    "            \n",
    "    return C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate all frequent itemsets for a given frequency threshold\n",
    "def apriori(dataset, freqThreshold):\n",
    "    # compute the number of instances in the dataset\n",
    "    N = len(dataset)\n",
    "    # compute the number items in the universe\n",
    "    d = len(dataset[0])\n",
    "\n",
    "    # Frequent itemsets will be stored in a dictionary. Key -- size of itemsets, Value -- all frequent itemsets of that size\n",
    "    F = {i+1 : [] for i in range(d)}\n",
    "\n",
    "    # Compute frequent 1-itemsets\n",
    "    for i in range(d):\n",
    "        if sup(dataset, [i]) >= freqThreshold:\n",
    "            F[1].append([i])\n",
    "\n",
    "    # For every itemsets size from 2 to d\n",
    "    for k in range(2,d+1):\n",
    "        # If there are no frequent (k-1)-itemsets, then STOP\n",
    "        if len(F[k-1]) == 0:\n",
    "            break\n",
    "        # Otherwise generate k-itemsets candidates\n",
    "        Ck = generateCandidates(F[k-1], k, d)\n",
    "        # and for every candidate k-itemset \n",
    "        for itemset in Ck:\n",
    "            # if it has sufficiently large support\n",
    "            if sup(dataset, itemset) >= freqThreshold:\n",
    "                # add it to the collections of the frequent k-itemsets\n",
    "                F[k].append(itemset)\n",
    "\n",
    "    return F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Generate and print all frequent itemsets for the frequency threshold 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F = apriori(dataset, 0.005)\n",
    "for k,val in F.items():\n",
    "    if len(val) > 0:\n",
    "        print('Frequent %d-itemset:' % (k), val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "\n",
    "1. Implement an algorithm that takes frequent itemsets as an input and generate from these itemsets all association rules at a given confidence threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate all subsets of a given set s, except the empty set and s itself (the function is implemented as generator, which means that we can use it as iterator)\n",
    "def powerset(s):\n",
    "    x = len(s)\n",
    "    masks = [1 << i for i in range(x)]\n",
    "    for i in range(1, (1 << x)-1):\n",
    "        yield [ss for mask, ss in zip(masks, s) if i & mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given frequent itemsets, generate the association rules at the given confidence threshold\n",
    "def generateRules(dataset, freqItemsets, confThreshold):\n",
    "    associationRules = []\n",
    "\n",
    "    # for every frequent itemset\n",
    "    for itemset in freqItemsets:\n",
    "        # of size at least 2\n",
    "        if len(itemset) >=2:\n",
    "            # and for every subset X of itemset\n",
    "            for X in powerset(itemset):\n",
    "                # if the confidence of X => Y (where Y is the complement of X) is at least the confidence threshold\n",
    "                support = sup(dataset, itemset)\n",
    "                conf = support/sup(dataset, X)\n",
    "                if conf >= confThreshold:\n",
    "                    # then generate association rule X => Y\n",
    "                    Y = [e for e in itemset if e not in X]\n",
    "                    associationRules.append((X,Y,support,conf))\n",
    "    return associationRules\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Generate and print all association rules at the frequency threshold 0.005 and the confidence threshold 0.8. For every rule, output its support and confidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print association rules\n",
    "def printAssociationRules(associationRules, itemNames):\n",
    "    i = 1\n",
    "    print(\"Support \\t\\t Confidence \\t\\t Association rule\")\n",
    "    for (X,Y,support,conf) in associationRules:\n",
    "        #print(\"%d. { %s } => { %s } \\t\\t\\t %.4f \\t\\t\\t %.4f\" % i, ','.join(itemNames[X]), ','.join(itemNames[Y]), support, conf)\n",
    "        print(\"%d. %.4f \\t\\t %.4f \\t\\t { %s } => { %s } \\t\\t \" % (i, support, conf, ', '.join(itemNames[X]), ', '.join(itemNames[Y])))\n",
    "        #print(i, '{',itemNames[X],'} => {', itemNames[Y],\"        \", support, )\n",
    "        i = i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqItemsets = []\n",
    "for kitemsets in F.values():\n",
    "    for itemset in kitemsets:\n",
    "        if len(itemset) > 0:\n",
    "            freqItemsets.append(itemset)\n",
    "\n",
    "associationRules = generateRules(dataset, freqItemsets, 0.8)\n",
    "print(\"Association Rules at the frequency threshold 0.005 and the confidence threshold 0.8\\n\")\n",
    "printAssociationRules(associationRules, itemNames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Now print only the association rules of the form X => {Survived} or X => {Not survived}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We are interested only in the association rules of the form X => {Survived} or X => {Not survived}\n",
    "associationRulesSurvived = []\n",
    "for (X,Y,support,conf) in associationRules:\n",
    "    if (Y == [8]) or (Y == [9]):\n",
    "        associationRulesSurvived.append((X,Y,support,conf))\n",
    "\n",
    "# Sort rules with respect to confidence\n",
    "associationRulesSurvived = sorted(associationRulesSurvived, key=lambda x: x[3], reverse=True)\n",
    "printAssociationRules(associationRulesSurvived, itemNames)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
